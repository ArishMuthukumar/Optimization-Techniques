{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**M ARISH**\n",
        "\n",
        "**SC25M159**\n",
        "\n",
        "Optimization Techniques Algorithms"
      ],
      "metadata": {
        "id": "7OAPFBCpgcIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Conjugate Gradient (for quadratic functions)**"
      ],
      "metadata": {
        "id": "EG3VEfLBeLx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def conjugate_gradient_quadratic(Q, b, x0, tol=1e-10, max_iter=1000, enable_history=True):\n",
        "    \"\"\"\n",
        "    Conjugate Gradient to minimize f(x) = 0.5 * (x^T Q x - x^T b)\n",
        "    which is equivalent to solving Q x = 0.5 b.\n",
        "\n",
        "    Inputs:\n",
        "      Q: SPD matrix (n x n)\n",
        "      b: vector (n,)\n",
        "      x0: initial guess (n,)\n",
        "    Returns:\n",
        "      x: solution\n",
        "      iters: number of iterations used\n",
        "      res_norm: final residual norm ||Qx - 0.5 b||\n",
        "      history: list of iterates (if enabled), else None\n",
        "    \"\"\"\n",
        "    x = x0.astype(float)\n",
        "    rhs = 0.5 * b  # b_std\n",
        "    r = rhs - Q @ x\n",
        "    p = r.copy()\n",
        "    rs_old = r @ r\n",
        "    history = [] if enable_history else None\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        Qp = Q @ p\n",
        "        denom = p @ Qp\n",
        "        if abs(denom) < 1e-14:\n",
        "            break\n",
        "        alpha = rs_old / denom\n",
        "\n",
        "        x = x + alpha * p\n",
        "        r = r - alpha * Qp\n",
        "\n",
        "        if enable_history:\n",
        "            history.append(x.copy())\n",
        "\n",
        "        rs_new = r @ r\n",
        "        if np.sqrt(rs_new) < tol:\n",
        "            return x, k + 1, np.linalg.norm(Q @ x - rhs), history\n",
        "\n",
        "        beta = rs_new / rs_old\n",
        "        p = r + beta * p\n",
        "        rs_old = rs_new\n",
        "\n",
        "    return x, max_iter, np.linalg.norm(Q @ x - rhs), history\n",
        "\n",
        "\n",
        "# Problem data (same as your steepest descent case)\n",
        "Q = np.array([[3, 0, 1],\n",
        "              [0, 4, 2],\n",
        "              [1, 2, 3]], dtype=float)\n",
        "b = np.array([3, 0, 1], dtype=float)\n",
        "\n",
        "# Start with any x0\n",
        "x0 = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "x_cg, iters, res_norm, hist = conjugate_gradient_quadratic(Q, b, x0, tol=1e-10, max_iter=1000, enable_history=True)\n",
        "print(\"CG Solution:\", x_cg)\n",
        "print(\"Iterations:\", iters)\n",
        "print(\"Final residual norm ||Qx - 0.5 b||:\", res_norm)\n",
        "\n",
        "# Compare to exact minimizer (solve Qx = 0.5 b)\n",
        "x_exact = np.linalg.solve(Q, 0.5 * b)\n",
        "print(\"Exact solution:\", x_exact)\n",
        "print(\"Error norm:\", np.linalg.norm(x_cg - x_exact))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_qPb0cX11rP",
        "outputId": "82ea19c8-2506-44ad-faf9-8703455801ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CG Solution: [5.00000000e-01 2.08166817e-17 0.00000000e+00]\n",
            "Iterations: 3\n",
            "Final residual norm ||Qx - 0.5 b||: 8.326672684688674e-17\n",
            "Exact solution: [0.5 0.  0. ]\n",
            "Error norm: 2.0816681711721685e-17\n"
          ]
        }
      ]
    }
  ]
}